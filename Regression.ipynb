{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functions \n",
    "import ot\n",
    "\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "cmap = cm.get_cmap('tab20c')\n",
    "y1_color = cmap(1/20) #blue\n",
    "y2_color = cmap(6/20) #orange\n",
    "y3_color = cmap(9/20) #green\n",
    "y4_color = cmap(14/20) #purple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st METHOD NON-ADAPTIVE TO THE COVARIATES X  \n",
    "### i.e. treating scores independently of X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines functions to compare MK quantiles and regions of prescribed shapes: ellipsoid / hyperrectangle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.patches import Rectangle \n",
    "\n",
    "def get_rectangle(scores,alpha): \n",
    "    marginal_alpha = np.sqrt(alpha) \n",
    "    center_outward_m = (1 - marginal_alpha)/2  \n",
    "    axis0 = np.quantile(scores.T[0],[1-center_outward_m,center_outward_m]) \n",
    "    axis1 = np.quantile(scores.T[1],[1-center_outward_m,center_outward_m]) \n",
    "    hyperrectangle = Rectangle((axis0[0],axis1[0]), axis0[1]-axis0[0], axis1[1]-axis1[0],\n",
    "                               fill=False,color=y2_color,linewidth=3,linestyle='--') \n",
    "    return(hyperrectangle)\n",
    "\n",
    "def get_vol_rectangle(scores,alpha):\n",
    "    marginal_alpha = np.sqrt(alpha)\n",
    "    center_outward_m = (1 - marginal_alpha)/2 \n",
    "    prod = 1 \n",
    "    for j in range(scores.shape[1]):\n",
    "        borders = np.quantile(scores.T[j],[1-center_outward_m,center_outward_m])\n",
    "        #print(borders)\n",
    "        prod = prod * (borders[0]-borders[1])\n",
    "    return(prod)\n",
    "\n",
    "\n",
    "def matrix_to_param(mat):\n",
    "    \"\"\"\n",
    "    Function taken from https://github.com/M-Soundouss/EllipsoidalConformalMTR/tree/main. \n",
    "    Calculates an ellipse's parameters to draw it as in https://cookierobotics.com/007/\n",
    "    :param mat: Covariance matrix\n",
    "    :return: Ellipse's parameters\n",
    "    \"\"\"\n",
    "    lambda1 = (mat[0, 0] + mat[1, 1]) / 2 + np.sqrt(\n",
    "        ((mat[0, 0] - mat[1, 1]) / 2) ** 2 + mat[0, 1] ** 2\n",
    "    )\n",
    "    lambda2 = (mat[0, 0] + mat[1, 1]) / 2 - np.sqrt(\n",
    "        ((mat[0, 0] - mat[1, 1]) / 2) ** 2 + mat[0, 1] ** 2\n",
    "    )\n",
    "\n",
    "    if mat[0, 1] == 0 and mat[0, 0] >= mat[1, 1]:\n",
    "        theta = 0\n",
    "    elif mat[0, 1] == 0 and mat[0, 0] < mat[1, 1]:\n",
    "        theta = np.pi / 2\n",
    "    else:\n",
    "        theta = np.arctan2(lambda1 - mat[0, 0], mat[0, 1])\n",
    "\n",
    "    return np.sqrt(lambda1), np.sqrt(lambda2), theta\n",
    "\n",
    "from math import degrees\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def get_ellipse(scores,alpha):\n",
    "    cov = np.cov(scores.T)\n",
    "    cov_1 = np.linalg.inv(cov)\n",
    "    MahalanobisNCM = -np.ones(len(scores)) # Mahalanobis Non-Conformity Measure\n",
    "    for k in range(len(scores)):\n",
    "        MahalanobisNCM[k] = scores[k].T @ cov_1 @ scores[k]\n",
    "    OrderStatistics = np.argsort(MahalanobisNCM)\n",
    "    q = alpha *(1+1/len(scores))\n",
    "    indexTreshold = OrderStatistics[(int(q*len(scores))+1)]\n",
    "    alpha_s = MahalanobisNCM[indexTreshold]\n",
    "    #print(np.mean(MahalanobisNCM <= alpha_s))\n",
    "    width, height, theta = matrix_to_param(cov * alpha_s )\n",
    "    ellipsis = Ellipse(\n",
    "            xy=(0,0),\n",
    "            width = 2 * width,\n",
    "            height = 2 * height,\n",
    "            angle = degrees(theta),\n",
    "            linewidth=3,\n",
    "            color=\"lightpink\",\n",
    "            fill=False\n",
    "        )\n",
    "    return(ellipsis)\n",
    "\n",
    "from scipy.special import gamma\n",
    "\n",
    "def get_vol_ellipse(scores,alpha):\n",
    "    cov = np.cov(scores.T)\n",
    "    cov_1 = np.linalg.inv(cov)\n",
    "    MahalanobisNCM = -np.ones(len(scores)) # Mahalanobis Non-Conformity Measure\n",
    "    for k in range(len(scores)):\n",
    "        MahalanobisNCM[k] = scores[k].T @ cov_1 @ scores[k]\n",
    "    OrderStatistics = np.argsort(MahalanobisNCM)\n",
    "    q = alpha *(1+1/len(scores))\n",
    "    indexTreshold = OrderStatistics[(int(q*len(scores))+1)]\n",
    "    alpha_s = MahalanobisNCM[indexTreshold]\n",
    "    d = scores.shape[1] # The number of dimensions\n",
    "    volume_unit_ball = np.pi**(d/2) / gamma(d/2 + 1)\n",
    "    return( np.linalg.det( cov*alpha_s )**(1/2) * volume_unit_ball )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example on simulated data, we consider the following mixture of Gaussians as the distribution of the noise around the prediction $\\hat{f}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_scores(n = 800):\n",
    "    S1 = np.array([[4,-3],[-3,4]])\n",
    "    S2 = np.array([[4,3],[3,4]])\n",
    "    S3 = np.array([[3,0],[0,1]])\n",
    "    Y1 = np.random.multivariate_normal([-5,0],S1,int(n*3/8))\n",
    "    Y2 = np.random.multivariate_normal([5,0],S2,int(n*3/8))\n",
    "    Y3 = np.random.multivariate_normal([0,-1],S3,int(n*2/8))\n",
    "    scores  = np.concatenate([Y1,Y2,Y3])\n",
    "    idx = np.arange(len(scores)) \n",
    "    np.random.shuffle(idx) \n",
    "    scores = scores[idx] \n",
    "    return(scores) \n",
    "\n",
    "def sample_scores(n = 800):\n",
    "    S1 = np.array([[6,-4],[-4,6]])\n",
    "    S2 = np.array([[6,4],[4,6]])\n",
    "    S3 = np.array([[2,0],[0,2]])\n",
    "    Y1 = np.random.multivariate_normal([-4.5,0],S1,int(n*1/3))\n",
    "    Y2 = np.random.multivariate_normal([4.5,0],S2,int(n*1/3))\n",
    "    Y3 = np.random.multivariate_normal([0,0],S3,int(n*1/3))\n",
    "    scores  = np.concatenate([Y1,Y2,Y3])\n",
    "    idx = np.arange(len(scores)) \n",
    "    np.random.shuffle(idx) \n",
    "    scores = scores[idx] \n",
    "    return(scores) \n",
    "\n",
    "np.random.seed(62)\n",
    "scores = sample_scores(n=1500)/10\n",
    "\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"white\")\n",
    "# Plot: \n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.scatter(scores[:,0],scores[:,1],c=\"black\",s=4)\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "\n",
    "# MK quantile region :\n",
    "Quantile_Treshold,mu,psi,psi_star,ranksMK,Score_treshold = functions.MultivQuantileTreshold(scores,alpha=alpha) # solve OT \n",
    "contourMK = functions.get_contourMK(Quantile_Treshold,psi_star,scores)\n",
    "print(\"volume of MK region:\", functions.get_volume_QR(Quantile_Treshold,mu,psi,scores) )\n",
    "\n",
    "# HYPERRECTANGLES : \n",
    "hyperrectangle = get_rectangle(scores,alpha)\n",
    "print(\"volume rectangle:\", get_vol_rectangle(scores,alpha) )\n",
    "\n",
    "# ELLIPSOIDAL :\n",
    "ellipsis = get_ellipse(scores,alpha)\n",
    "print(\"volume ellipse:\", get_vol_ellipse(scores,alpha) )\n",
    "\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "# Plot: \n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "df = pd.DataFrame(scores,columns=[\"X1\",\"X2\"])\n",
    "sns.scatterplot(df,x=\"X1\",y=\"X2\",c=\"black\",s=15,alpha=0.5)#,c=ranksMK) \n",
    "plt.plot(contourMK.T[0],contourMK.T[1],color='cornflowerblue',linewidth=3)\n",
    "#plt.scatter(Score_treshold[0],Score_treshold[1],c=\"red\",s=100,marker=\"+\") \n",
    "ax.add_patch(hyperrectangle)\n",
    "ax.add_patch(ellipsis)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"quantiles_scores.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the functions to compute volumes, we can repeat experiments to compare efficiency and coverage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of coverage and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LearnParameters(scores,alpha):\n",
    "    ''' Learn parameters for MK quantiles, hyperrectangles, and ellipsoids. scores = data on which quantiles are computed. alpha = quantile level. '''\n",
    "    #############################################\n",
    "    ### MK : \n",
    "    Quantile_Treshold,mu,psi,psi_star,ranksMK,Score_treshold = functions.MultivQuantileTreshold(scores,alpha=alpha)\n",
    "    ############################################# \n",
    "    ### Ellipsoids: \n",
    "    cov = np.cov(scores.T)\n",
    "    cov_1 = np.linalg.inv(cov)\n",
    "    MahalanobisNCM = -np.ones(len(scores)) # Mahalanobis Non-Conformity Measure\n",
    "    for k in range(len(scores)):\n",
    "        MahalanobisNCM[k] = scores[k].T @ cov_1 @ scores[k]\n",
    "    OrderStatistics = np.argsort(MahalanobisNCM)\n",
    "    q = alpha *(1+1/len(scores))\n",
    "    indexTreshold = OrderStatistics[(int(q*len(scores))+1)]\n",
    "    alpha_s = MahalanobisNCM[indexTreshold]\n",
    "\n",
    "    ############################################# \n",
    "    ### Hyperrectangles : \n",
    "    componentwise_alpha = np.sqrt(alpha) \n",
    "    center_outward_m = (1 - componentwise_alpha)/2  \n",
    "    list_axis = []\n",
    "    for k in range(scores.shape[1]):\n",
    "        axis = np.quantile(scores.T[k],[1-center_outward_m,center_outward_m]) \n",
    "        list_axis.append(axis) \n",
    "    list_axis = np.array(list_axis)\n",
    "    return(Quantile_Treshold,mu,psi,psi_star,ranksMK,Score_treshold,alpha_s,cov,cov_1,list_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "alpha = 0.9\n",
    "\n",
    "#############################################\n",
    "rates_cvg_MK = []\n",
    "rates_cvg_rect = []\n",
    "rates_cvg_ell = []\n",
    "volumes_MK = []\n",
    "volumes_rect = []\n",
    "volumes_ell = [] \n",
    "\n",
    "for rep in range(50): \n",
    "    t0 = time()\n",
    "    print(rep)\n",
    "    # Sample calibration data \n",
    "    scores_cal = sample_scores(n=1000)/10 \n",
    "    Quantile_Treshold,mu,psi,psi_star,ranksMK,Score_treshold,alpha_s,cov,cov_1,list_axis = LearnParameters(scores_cal,alpha)\n",
    "\n",
    "    scores_test = sample_scores(n=1000)/10 \n",
    "    # Compute MK quantile region coverage\n",
    "    RankMK = functions.RankFunc(scores_test, mu, psi) \n",
    "    norm_RankMK = np.linalg.norm(RankMK, axis=1)\n",
    "    coverageMK = np.mean(norm_RankMK <= Quantile_Treshold)\n",
    "\n",
    "    # Compute hyperrectangle coverage\n",
    "    within_lower_bounds = np.all(list_axis.T[1] <= scores_test, axis=1)\n",
    "    within_upper_bounds = np.all(scores_test <= list_axis.T[0], axis=1)\n",
    "    coverageRectangle = np.mean(within_lower_bounds & within_upper_bounds)\n",
    "\n",
    "    # Compute Ellipse coverage\n",
    "    MahalanobisNCM = np.einsum('ij,ji->i', scores_test @ cov_1, scores_test.T)\n",
    "    # Efficient Mahalanobis calculation\n",
    "    coverageEllipse = np.mean(MahalanobisNCM <= alpha_s)\n",
    "    # Gather results\n",
    "    rates_cvg_MK.append(coverageMK)\n",
    "    rates_cvg_rect.append(coverageRectangle)\n",
    "    rates_cvg_ell.append(coverageEllipse)\n",
    "\n",
    "    # Volumes :\n",
    "    # MK :\n",
    "    volumes_MK.append(functions.get_volume_QR(Quantile_Treshold,mu,psi,scores_test,N = len(scores_test))) \n",
    "    # Rectangle: \n",
    "    lengths_sides = list_axis.T[0]-list_axis.T[1]\n",
    "    volumes_rect.append( np.prod(lengths_sides)  )\n",
    "    # Ellipse: \n",
    "    d = scores_test.shape[1] # The number of dimensions\n",
    "    volume_unit_ball = np.pi**(d/2) / gamma(d/2 + 1)\n",
    "    volumes_ell.append( np.linalg.det( cov*alpha_s )**(1/2) * volume_unit_ball )\n",
    "    print(np.round(time()-t0,3) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_cvg_MK = np.array(rates_cvg_MK)  \n",
    "rates_cvg_rect = np.array(rates_cvg_rect)  \n",
    "rates_cvg_ell = np.array(rates_cvg_ell)  \n",
    "volumes_MK = np.array(volumes_MK)  \n",
    "volumes_rect = np.array(volumes_rect)  \n",
    "volumes_ell = np.array(volumes_ell)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "plt.figure()\n",
    "g =sns.catplot( \n",
    "    data=df,  kind=\"bar\",palette=[\"cornflowerblue\",y2_color,\"lightpink\"],aspect=.8,capsize=0.3\n",
    "        #x=\"Method\", y=\"Coverage\", palette=[\"cornflowerblue\",y2_color,\"lightpink\"],height=2,width=0.8,capsize=0.3,aspect=1 #,hue_order=[\"IP\",\"MS\",\"APS\",\"OTCP\"]\n",
    ")   \n",
    "plt.hlines(alpha,xmin=-0.6,xmax=2.6,linewidth=3,linestyles=\"dashed\",color=\"black\") \n",
    "plt.ylim(0,1)\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.ylabel(\"Marginal coverage\") \n",
    "plt.savefig(\"Coverage_reg_simu.pdf\", format=\"pdf\",bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure()\n",
    "g =sns.catplot( \n",
    "    data=df,  kind=\"bar\",palette=[\"cornflowerblue\",y2_color,\"lightpink\"],aspect=0.8,capsize=0.3\n",
    "        #x=\"Method\", y=\"Coverage\", palette=[\"cornflowerblue\",y2_color,\"lightpink\"],height=2,width=0.8,capsize=0.3,aspect=1 #,hue_order=[\"IP\",\"MS\",\"APS\",\"OTCP\"]\n",
    ")   \n",
    "sns.despine(trim=True, left=True)\n",
    "plt.ylabel(\"Volume of prediction sets\")  \n",
    "plt.savefig(\"Efficiency_reg_simu.pdf\", format=\"pdf\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell represents predictions together with the quantile region centered at this prediction, which is is related to the univariate \n",
    "$$\n",
    "[\\hat{f}(x)-q_\\alpha, \\hat{f}(x) + q_\\alpha]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the previous computations have been made on scores (independently of $X$). \n",
    "#### This amounts to suppose that the noise is independent from $X$. \n",
    "#### The next cell uses codes for quantile regression to visualize the impact in terms of prediction regions (around a prediction $\\hat{f}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def QuantileKn_d(x, y, n, x_tick,k,levels):\n",
    "    ''' compute MK quantile contours conditional on x_tick, based on the calibration data x,y  '''\n",
    "    ### x=covariates \n",
    "    ### y=output variables\n",
    "    ### n= number of neighbors \n",
    "    ### x_tick = list of points x where the conditional quantile function Q( . / X = x) is to be computed \n",
    "    ### k number of points in quantile contour\n",
    "    ### levels : contour levels to be computed, each between 0 and 1 \n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)    \n",
    "\n",
    "    ## uniform grid\n",
    "    d = y.shape[1]\n",
    "    U = functions.sample_grid(np.zeros((n,d)))\n",
    "\n",
    "    ## Compute quantile contours, for each x\n",
    "    for i in range(len(x_tick)):\n",
    "        X = np.zeros(n)\n",
    "        ### select n-nearest neighbors\n",
    "        order = np.argsort(np.linalg.norm(x-x_tick[i],axis=1) )\n",
    "        X = x[order][:n]\n",
    "        Y = y[order][:n]\n",
    "\n",
    "        psi_star = functions.OT_exact_(U,Y) ## POT with scaling/rescaling  \n",
    "        \n",
    "        quantile_contours = []\n",
    "        for alpha in levels:\n",
    "            sphere = np.random.multivariate_normal(np.zeros(d),np.eye(d),k)\n",
    "            sphere = sphere / np.linalg.norm(sphere,axis=1).reshape((k,1))\n",
    "            quantile_contours.append( functions.T0(alpha*sphere , Y ,psi_star) )\n",
    "        quantile_contours = np.array(quantile_contours)\n",
    "    return(quantile_contours)\n",
    "\n",
    "\n",
    "\n",
    "def PlotQuantile3D(x , y, z,quantile, quantile2,quantile3,center,NumberOfGrid,ContourLength):\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter3D(x, y, z, color='gray',s=30,alpha=0.2,label=\"Test samples\")\n",
    "    ax.set_xlabel('X',fontsize=15)\n",
    "    ax.set_ylabel('$Y_1$',fontsize=15)\n",
    "    ax.set_zlabel('$Y_2$',fontsize=15)\n",
    "    #Plot the centers\n",
    "    ax.plot3D(center[:,0],center[:,1],center[:,2], 'cornflowerblue',alpha = 1, \n",
    "              linewidth=3.0,linestyle='-.',zorder=5,label=\"Prediction $\\hat{f}(x)$\")\n",
    "    for k in range(NumberOfGrid):\n",
    "        si=ContourLength\n",
    "        r1=k*(si)\n",
    "        r2=(si)*(k+1)\n",
    "        q1=np.zeros((si+1,3))\n",
    "        q1[0:si,:]=quantile[r1:r2,:]\n",
    "        q1[si,:]=quantile[r1,:]\n",
    "        q1=np.zeros((si+1,3))\n",
    "        q1[0:si,:]=quantile2[r1:r2,:]\n",
    "        q1[si,:]=quantile2[r1,:]\n",
    "        q1=np.zeros((si+1,3))\n",
    "        q1[0:si,:]=quantile3[r1:r2,:]\n",
    "        q1[si,:]=quantile3[r1,:]\n",
    "        if k==0:\n",
    "            ax.plot3D(q1[:,0],q1[:,1],q1[:,2],'cornflowerblue',alpha = 1, linewidth=2.0,zorder=2,label=\"MK prediction sets\")\n",
    "        else:\n",
    "            ax.plot3D(q1[:,0],q1[:,1],q1[:,2],'cornflowerblue',alpha = 1, linewidth=2.0,zorder=2)\n",
    "    ax.set_box_aspect(None, zoom=0.9)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(0.13, 0.75))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 999\n",
    "x = np.linspace(0,2,n_sample)\n",
    "noise = sample_scores(n_sample)/2\n",
    "\n",
    "Y = np.array([ 2*x**2, (x+1)**2]).T + noise\n",
    "y = Y.T[0]\n",
    "z = Y.T[1]\n",
    "k = 100 # number of points in a contour \n",
    "grid = 6\n",
    "levels = np.array([0.2,0.5,0.8])\n",
    "\n",
    "### predictor\n",
    "BayesRegressor = np.array([x, 2*x**2, (x+1)**2]).T \n",
    "\n",
    "### Quantile computation \n",
    "d = Y.shape[1]\n",
    "scores = Y - BayesRegressor[:,1:]\n",
    "U = functions.sample_grid(scores)\n",
    "psi, psi_star = functions.learn_psi(U,scores)\n",
    "\n",
    "sphere = np.array([np.cos(2*np.pi*np.arange(k)/k),np.sin(2*np.pi*np.arange(k)/k)]).T\n",
    "quantile_contours = []\n",
    "for alpha in levels:\n",
    "    quantile_contours.append( functions.T0(alpha*sphere , scores ,psi_star) )\n",
    "quantile_contours = np.array(quantile_contours)\n",
    "\n",
    "### x mesh\n",
    "indices_grid = np.array( np.linspace(0,0.999,grid)*len(x) ,dtype=int)\n",
    "x_tick = x[indices_grid]\n",
    "\n",
    "\n",
    "quantiles = []\n",
    "for i in range(len(levels)):\n",
    "    g = 0\n",
    "    Q0 = quantile_contours[i] + BayesRegressor[:,1:][indices_grid][g] \n",
    "    BayesRegressor[indices_grid] \n",
    "    quantile1 = np.array([np.repeat(x_tick[g] ,k),Q0.T[0],Q0.T[1]]).T \n",
    "    for g in range(1,grid):\n",
    "        Q0 = quantile_contours[i] + BayesRegressor[:,1:][indices_grid][g] \n",
    "        Q0x = np.array([np.repeat(x_tick[g] ,k),Q0.T[0],Q0.T[1]]).T \n",
    "        quantile1 = np.concatenate([quantile1,Q0x])\n",
    "    quantiles.append(quantile1)\n",
    "quantile1, quantile2, quantile3 = quantiles \n",
    "\n",
    "PlotQuantile3D(x,y,z,quantile1, quantile2, quantile3, BayesRegressor,grid,k) \n",
    "plt.savefig(\"HomoscedasticNoise.pdf\", format=\"pdf\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We now turn to adaptive regions via quantile regression on scores \n",
    " \n",
    " # Heteroscedastic noise, the general case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with an example with $X\\in\\mathbb{R}$ and $Y\\in\\mathbb{R}^2$, in order to vizualise the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ConformalQuantileKn(x, y, n, x_tick,k,alpha):\n",
    "    ''' To plot quantile contours with 3D visualisation '''\n",
    "    ### x=covariates \n",
    "    ### y=output variables\n",
    "    ### n= number of neighbors \n",
    "    ### x_tick = list of points x where the conditional quantile function Q( . / X = x) is to be computed \n",
    "    ### k number of points in quantile contour\n",
    "    ### levels : contour levels to be computed, each between 0 and 1 \n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)    \n",
    "\n",
    "    ## uniform grid\n",
    "    d = y.shape[1]\n",
    "    U = functions.sample_grid(np.zeros((n,d)))\n",
    "\n",
    "    quantile_contours = []\n",
    "    ## Compute quantile contours, for each x\n",
    "    for i in range(len(x_tick)):\n",
    "        X = np.zeros(n)\n",
    "        ### select n-nearest neighbors\n",
    "        order = np.argsort(np.linalg.norm(x-x_tick[i],axis=1) )\n",
    "        scores = scores[order][:n]\n",
    "\n",
    "        Quantile_Treshold,mu,psi,psi_star,ranksMK,Score_treshold = functions.MultivQuantileTreshold(scores,alpha=alpha)\n",
    "        \n",
    "        sphere = np.array([np.cos(2*np.pi*np.arange(k)/k),np.sin(2*np.pi*np.arange(k)/k)]).T\n",
    "        quantile_contours.append( functions.T0(Quantile_Treshold*sphere , scores ,psi_star) ) \n",
    "    return(np.array(quantile_contours)) \n",
    "\n",
    "def get_metricsMK(newdata,mu,psi,Quantile_Treshold,scores):\n",
    "    # Compute MK quantile region coverage :\n",
    "    RanksMK = functions.RankFunc(newdata, mu, psi) \n",
    "    norm_RankMK = np.linalg.norm(RanksMK, axis=1)\n",
    "    coverageMK = np.mean(norm_RankMK <= Quantile_Treshold)\n",
    "    # Compute volume of MK quantile region :\n",
    "    volumeMK = functions.get_volume_QR(Quantile_Treshold,mu,psi,scores) \n",
    "    return(coverageMK,volumeMK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 999\n",
    "x = np.linspace(0,2,n_sample)\n",
    "noise2 = sample_scores(n_sample)/2\n",
    "Y = np.array([ 2*x**2, (x+1)**2]).T  + noise2*np.sqrt(x).reshape(n_sample,1)\n",
    "y = Y.T[0] \n",
    "z = Y.T[1] \n",
    "n = 100 # number of neighbors\n",
    "\n",
    "k = 100 # number of points in a contour \n",
    "grid = 6\n",
    "alpha = 0.9\n",
    "\n",
    "### predictor\n",
    "BayesRegressor = np.array([x, 2*x**2, (x+1)**2]).T \n",
    "\n",
    "### Quantile computation \n",
    "d = Y.shape[1] \n",
    "U = functions.sample_grid(np.zeros((n,d)))  \n",
    " \n",
    "### x mesh\n",
    "indices_grid = np.array( np.linspace(0,0.999,grid)*len(x) ,dtype=int) \n",
    "x_tick = x[indices_grid] \n",
    "\n",
    "sphere = np.array([np.cos(2*np.pi*np.arange(k)/k),np.sin(2*np.pi*np.arange(k)/k)]).T \n",
    "quantile_contours = [] \n",
    "for i in range(grid): \n",
    "    ### select n-nearest neighbors  \n",
    "    scores = Y - BayesRegressor[:,1:] \n",
    "    order = np.argsort(np.abs(x-x_tick[i]) ) \n",
    "    scores = scores[order][:n] \n",
    "    # Solve OT \n",
    "    Quantile_Treshold,mu,psi,psi_star,ranksMK,Score_treshold = functions.MultivQuantileTreshold(scores,alpha=alpha)\n",
    "    # Compute quantile contour \n",
    "    Q0 = functions.T0(Quantile_Treshold*sphere , scores ,psi_star) \n",
    "    quantile_contours.append( Q0 ) \n",
    "quantile_contours = np.array(quantile_contours) \n",
    "\n",
    "quantiles = []\n",
    "g = 0 \n",
    "Q0 = quantile_contours[g] + BayesRegressor[:,1:][indices_grid][g] \n",
    "BayesRegressor[indices_grid] \n",
    "quantile1 = np.array([np.repeat(x_tick[g] ,k),Q0.T[0],Q0.T[1]]).T \n",
    "for g in range(1,grid):\n",
    "    Q0 = quantile_contours[g] + BayesRegressor[:,1:][indices_grid][g] \n",
    "    Q0x = np.array([np.repeat(x_tick[g] ,k),Q0.T[0],Q0.T[1]]).T \n",
    "    quantile1 = np.concatenate([quantile1,Q0x])\n",
    "\n",
    "quantile1, quantile2, quantile3 = quantile1,quantile1,quantile1 \n",
    "\n",
    "PlotQuantile3D(x,y,z,quantile1, quantile2, quantile3, BayesRegressor,grid,k) \n",
    "plt.savefig(\"HeteroscedasticNoise.pdf\", format=\"pdf\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate conditional coverage on this toy-example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### SAMPLE CALIBRATION DATA \n",
    "n_sample = 999 \n",
    "\n",
    "alpha = 0.9 \n",
    "Nrep = 100 # Proportion of test data in the prediction region is repeated `Nrep`` times \n",
    "n_test = 999 # n_test new data points in testing data \n",
    "k = 100 #  # number of neigbors to compute Quantile regression function   \n",
    "\n",
    "########################################################\n",
    "####### Assess coverage on an interval (a,b)\n",
    "########################################################\n",
    "a,b = 0.25,0.5 # interval on which to sample \n",
    "res = - np.ones(Nrep)\n",
    "for rep in range(Nrep):\n",
    "    ####### SAMPLE CALIBRATION DATA \n",
    "    x = np.linspace(0,2,n_sample) \n",
    "    noise2 = sample_scores(n_sample)/2 \n",
    "    y = np.array([ 2*x**2, (x+1)**2]).T  + noise2*np.sqrt(x).reshape(n_sample,1)\n",
    "    BayesRegressor = np.array([2*x**2, (x+1)**2]).T \n",
    "    scores = y - BayesRegressor \n",
    "    x = x.reshape(-1, 1) \n",
    "\n",
    "    ####### SAMPLE TEST DATA IN THE INTERVAL (a,b) and compute scores on this test data\n",
    "    X_test = np.linspace(a,b,n_test)\n",
    "    noise2 = sample_scores(n_test)/2\n",
    "    Y_test =  np.array([ 2*X_test**2, (X_test+1)**2]).T  + noise2*np.sqrt(X_test).reshape(n_test,1)\n",
    "    BayesRegressor = np.array([2*X_test**2, (X_test+1)**2]).T \n",
    "    Stest = Y_test - BayesRegressor\n",
    "\n",
    "    ####### Conformal multivariate regression \n",
    "    coverage_conditional_X_test = []\n",
    "    for i,x_tick in enumerate(X_test): # for each new testing point x_tick \n",
    "        Quantile_Treshold, mu, psi,psi_star = functions.ConformalQuantileReg(x,scores, k, x_tick,alpha) # compute conditional rank map on calibration data\n",
    "        Ranks_Stest = functions.RankFunc(Stest[i],mu,psi)  # apply this conditional rank map to the score associated to x_tick \n",
    "        coverage_conditional_X_test.append( 1*(np.linalg.norm(Ranks_Stest) <= Quantile_Treshold) ) # verify if the test data points belongs to the quantile region\n",
    "    coverage_conditional_X_test = np.array(coverage_conditional_X_test) \n",
    "    res[rep] = np.mean( coverage_conditional_X_test )\n",
    "res1 = res\n",
    "\n",
    "print(\"1 ok \")\n",
    "\n",
    "########################################################\n",
    "####### Assess coverage on an another interval (a,b)\n",
    "########################################################\n",
    "a,b = 1.5,2 # interval on which to sample \n",
    "res = - np.ones(Nrep)\n",
    "for rep in range(Nrep):\n",
    "    ####### SAMPLE CALIBRATION DATA \n",
    "    x = np.linspace(0,2,n_sample) \n",
    "    noise2 = sample_scores(n_sample)/2 \n",
    "    y = np.array([ 2*x**2, (x+1)**2]).T  + noise2*np.sqrt(x).reshape(n_sample,1)\n",
    "    BayesRegressor = np.array([2*x**2, (x+1)**2]).T \n",
    "    scores = y - BayesRegressor \n",
    "    x = x.reshape(-1, 1) \n",
    "    ####### SAMPLE TEST DATA IN THE INTERVAL (a,b) and compute scores on this test data\n",
    "    X_test = np.linspace(a,b,n_test)\n",
    "    noise2 = sample_scores(n_test)/2\n",
    "    Y_test =  np.array([ 2*X_test**2, (X_test+1)**2]).T  + noise2*np.sqrt(X_test).reshape(n_test,1)\n",
    "    BayesRegressor = np.array([2*X_test**2, (X_test+1)**2]).T \n",
    "    Stest = Y_test - BayesRegressor\n",
    "\n",
    "    ####### Conformal multivariate regression \n",
    "    coverage_conditional_X_test = []\n",
    "    for i,x_tick in enumerate(X_test): # for each new testing point x_tick \n",
    "        Quantile_Treshold, mu, psi,psi_star = functions.ConformalQuantileReg(x,scores, k, x_tick,alpha) # compute conditional rank map on calibration data\n",
    "        Ranks_Stest = functions.RankFunc(Stest[i],mu,psi)  # apply this conditional rank map to the score associated to x_tick \n",
    "        coverage_conditional_X_test.append( 1*(np.linalg.norm(Ranks_Stest) <= Quantile_Treshold) ) # verify if the test data points belongs to the quantile region\n",
    "    coverage_conditional_X_test = np.array(coverage_conditional_X_test) \n",
    "    res[rep] = np.mean( coverage_conditional_X_test )\n",
    "res2 = res \n",
    "\n",
    "print(\"2 ok \")\n",
    "\n",
    "########################################################\n",
    "####### Assess coverage on an another interval (a,b)\n",
    "########################################################\n",
    "a,b = 0,2 # interval on which to sample \n",
    "res = - np.ones(Nrep)\n",
    "for rep in range(Nrep):\n",
    "    ####### SAMPLE CALIBRATION DATA \n",
    "    x = np.linspace(0,2,n_sample) \n",
    "    noise2 = sample_scores(n_sample)/2 \n",
    "    y = np.array([ 2*x**2, (x+1)**2]).T  + noise2*np.sqrt(x).reshape(n_sample,1)\n",
    "    BayesRegressor = np.array([2*x**2, (x+1)**2]).T \n",
    "    scores = y - BayesRegressor \n",
    "    x = x.reshape(-1, 1) \n",
    "    ####### SAMPLE TEST DATA IN THE INTERVAL (a,b) and compute scores on this test data\n",
    "    X_test = np.linspace(a,b,n_test)\n",
    "    noise2 = sample_scores(n_test)/2\n",
    "    Y_test =  np.array([ 2*X_test**2, (X_test+1)**2]).T  + noise2*np.sqrt(X_test).reshape(n_test,1)\n",
    "    BayesRegressor = np.array([2*X_test**2, (X_test+1)**2]).T \n",
    "    Stest = Y_test - BayesRegressor\n",
    "\n",
    "    ####### Conformal multivariate regression \n",
    "    coverage_conditional_X_test = []\n",
    "    for i,x_tick in enumerate(X_test): # for each new testing point x_tick \n",
    "        Quantile_Treshold, mu, psi,psi_star = functions.ConformalQuantileReg(x,scores, k, x_tick,alpha) # compute conditional rank map on calibration data\n",
    "        Ranks_Stest = functions.RankFunc(Stest[i],mu,psi)  # apply this conditional rank map to the score associated to x_tick \n",
    "        coverage_conditional_X_test.append( 1*(np.linalg.norm(Ranks_Stest) <= Quantile_Treshold) ) # verify if the test data points belongs to the quantile region\n",
    "    coverage_conditional_X_test = np.array(coverage_conditional_X_test) \n",
    "    res[rep] = np.mean( coverage_conditional_X_test )\n",
    "resmarginal = res \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure()\n",
    "g =sns.catplot( \n",
    "    data=df,  kind=\"bar\",palette=[\"cornflowerblue\",\"cornflowerblue\",\"cornflowerblue\"],aspect=.8,capsize=0.3\n",
    "        #x=\"Method\", y=\"Coverage\", palette=[\"cornflowerblue\",y2_color,\"lightpink\"],height=2,width=0.8,capsize=0.3,aspect=1 #,hue_order=[\"IP\",\"MS\",\"APS\",\"OTCP\"]\n",
    ")   \n",
    "plt.hlines(alpha,xmin=-0.6,xmax=2.6,linewidth=3,linestyles=\"dashed\",color=\"black\") \n",
    "plt.ylim(0,1)\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.ylabel(\"Coverage\") \n",
    "plt.savefig(\"SimuConditionalCVG.pdf\", format=\"pdf\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Real data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### LOAD DATA \n",
    "\n",
    "list_data = [\"enb\",\"atp1d\",\"jura\",\"rf1\",\"wq\",\"scm20d\",\"oes10\",\"oes97\"] \n",
    "dim_targets =  [2,6,7,8,14,16,16,16]\n",
    "# Respective number of points : 768, 337, 359, 9005, 1060, 8966, 403,334\n",
    "# datasets in increasing number of size : [334,337,359,403,768,1060,8966,9005]\n",
    "list_data = [\"oes97\",\"atp1d\",\"jura\",\"oes10\",\"enb\",\"wq\",\"scm20d\",\"rf1\"] \n",
    "dim_targets =  [16,6,7,16,2,14,16,8]\n",
    "\n",
    "m = 1\n",
    "arff_file = arff.loadarff('./data/{}.arff'.format(list_data[m])) \n",
    "df = pd.DataFrame(arff_file[0]).dropna()\n",
    "print(df.head()) \n",
    "\n",
    "## Extract target variable y \n",
    "df = np.array(df) \n",
    "tt = df.shape[1] - dim_targets[m]\n",
    "x = df[:,:tt] \n",
    "y = df[:,tt:] \n",
    "print(\"Amount of data:\", x.shape,y.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next cell defines the needed functions for MK conditional quantiles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_content_conditionalQR(xtest,stest,x,scores,alpha=0.9,n=100):\n",
    "    ''' xtest is one observation from X-space, and stest is the associated ground-truth score.\n",
    "     x,y,scores are the calibration data with previously computed scores. required for knn step. n = number of neighbors for conditional QR. '''\n",
    "    ########### get conditional quantiles and ranks \n",
    "    Quantile_Treshold, mu, psi,psi_star = functions.ConformalQuantileReg(x,scores, n=n, x_tick=xtest,alpha=alpha) \n",
    "    volume_QR = functions.get_volume_QR(Quantile_Treshold,mu,psi,scores) \n",
    "    ########### evaluate our prediction region by verifying if it contains the ground truth \n",
    "    Ranks_Stest = functions.RankFunc(stest,mu,psi)\n",
    "    indic_content = 1*(np.linalg.norm(Ranks_Stest) <= Quantile_Treshold) \n",
    "    return indic_content,volume_QR\n",
    "\n",
    "def SetCoverage(x,scores,Xtest,Stest,alpha=0.9,n=100):\n",
    "    ''' Run test_content_conditionalQR over the dataset Xtest.  \n",
    "    - x,scores correspond to calibration data, to learn MK quantiles on it. \n",
    "    Returns the proportion of Ytest that are effectively in our conditional prediction set. '''\n",
    "    prop = 0\n",
    "    average_volume = 0\n",
    "    for i in range(len(Xtest)):\n",
    "        xtest = Xtest[i] \n",
    "        stest = Stest[i] \n",
    "        indic_content,volume_QR = test_content_conditionalQR(xtest,stest,x,scores,alpha=alpha,n=n)\n",
    "        prop += indic_content \n",
    "        average_volume += volume_QR\n",
    "    prop = prop / len(Xtest)\n",
    "    average_volume = average_volume / len(Xtest)\n",
    "    return(prop,average_volume)\n",
    "\n",
    "def WorstSetCoverage(x,scores,Xtest,Stest,alpha=0.9,n=100,n_neighbors=100):\n",
    "    '''\n",
    "    This functions selects 5 random subsets Ai of Xtest, on which it estimates conditional coverage, before taking the minimal coverage value (the worst case scenario). \n",
    "    Each subset Ai is selected by a random choice of point (and its n neighbors) within Xtest. \n",
    "\n",
    "    - x,y,scores shall come from calibration data : they are used for computation of MK quantiles \n",
    "    - Xtest and Stest = test data, on which to evaluate empirical coverage  \n",
    "    - n = number of points in subset Ai \n",
    "    - n_neighbors = number of neighbors to compute MK quantile regression,  \n",
    "    '''\n",
    "    # choice of some points in Xtest, that will serve as centers to define subsets \n",
    "    random_idx = np.random.randint(len(Xtest),size=5) \n",
    "    # For each, define a neighborood Ai and compute empirical coverage on it \n",
    "    list_coverage_Ai = - np.ones(len(random_idx))\n",
    "    list_average_volume_Ai = - np.ones(len(random_idx))\n",
    "    for i,idx in enumerate(random_idx):\n",
    "        ### select nn-nearest neighbors\n",
    "        if len(x.shape)>1:\n",
    "            norms_ranks = np.linalg.norm(Xtest-Xtest[idx],axis=1)\n",
    "        else:\n",
    "            norms_ranks = np.abs(Xtest-Xtest[idx])\n",
    "        order = np.argsort(norms_ranks )\n",
    "        Ai = Xtest[order][:n] \n",
    "        Scores_Ai = Stest[order][:n] \n",
    "        Coverage_Ai,average_volume = SetCoverage(x,scores,Xtest=Ai,Stest=Scores_Ai,alpha=alpha,n=n_neighbors)\n",
    "        list_coverage_Ai[i] = Coverage_Ai\n",
    "        list_average_volume_Ai[i] = average_volume\n",
    "\n",
    "    return( np.min(list_coverage_Ai),list_average_volume_Ai )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for ellispoids :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ellipsoidal_conformal_utilities import (\n",
    "    ellipsoidal_non_conformity_measure,\n",
    "    ellipse_global_alpha_s,\n",
    "    ellipse_local_alpha_s,\n",
    "    local_ellipse_validity_efficiency,\n",
    "    ellipse_volume\n",
    ")\n",
    "\n",
    "def get_params_localEllipsoids(X_cal,y_cal,y_pred_cal,alpha,n_neighbors):\n",
    "    ''' For confidence level alpha, learn mixture of local and global covariance on  '''\n",
    "    lam = 0.95 # lambda parameter for mixing local and global covariance\n",
    "    cov_train, alpha_s = ellipse_global_alpha_s(\n",
    "        y_true_train = y_cal,\n",
    "        y_pred_train = y_pred_cal, \n",
    "        y_true_cal = y_cal, \n",
    "        y_pred_cal = y_pred_cal, \n",
    "        epsilon = 1-alpha\n",
    "    )\n",
    "    knn, local_alpha_s = ellipse_local_alpha_s(\n",
    "        x_train = X_cal,\n",
    "        x_cal = X_cal,\n",
    "        y_true_train = y_cal,\n",
    "        y_pred_train = y_pred_cal,\n",
    "        y_true_cal = y_cal,\n",
    "        y_pred_cal = y_pred_cal,\n",
    "        epsilon = 1-alpha,\n",
    "        n_neighbors = n_neighbors,\n",
    "        lam = lam,\n",
    "        cov_train = cov_train\n",
    "    )\n",
    "    return(cov_train,knn, local_alpha_s )\n",
    "\n",
    "def SetCoverage_ell(X_cal,y_cal,y_pred_cal,X_test,y_test,y_pred_test,alpha,n_neighbors):\n",
    "    # 1) KNN step : get 'n_neighbors' nearest neighbors of each X_test[i] in X_cal\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_cal)\n",
    "    local_neighbors_test = knn.kneighbors(X_test, return_distance=False) \n",
    "\n",
    "    # 2) learn local ang global covariance on calibration data, and get the calibrated treshold: 'local_alpha_s'\n",
    "    cov_train,knn, local_alpha_s = get_params_localEllipsoids(X_cal,y_cal,y_pred_cal,alpha,n_neighbors) \n",
    "    \n",
    "    # 3) compute validity on test data :\n",
    "    ### y_cal and  y_pred_cal are required to compute calibration scores and extract the indices corresponding to 'local_neighbors_test'\n",
    "    normalized_ellipse_validity, normalized_ellipse_efficiency, = local_ellipse_validity_efficiency(\n",
    "                local_neighbors_test=local_neighbors_test,\n",
    "                y_true_test = y_test,\n",
    "                y_pred_test = y_pred_test,\n",
    "                y_true_train = y_cal,\n",
    "                y_pred_train = y_pred_cal,\n",
    "                local_alpha_s = local_alpha_s,\n",
    "                dim=y_cal.shape[1],\n",
    "                lam=0.95,\n",
    "                cov_train = cov_train\n",
    "        )\n",
    "    return( normalized_ellipse_validity/100, normalized_ellipse_efficiency )\n",
    "\n",
    "def WorstSetCoverage_Ellipsoids(x,y,Xtest,Ytest,parameters_ell,nn=100):\n",
    "    '''\n",
    "    - x,y,scores shall come from calibration data : they are used for computation of MK quantiles \n",
    "    - Xtest and Stest = test data, on which to evaluate empirical coverage  \n",
    "    - nn = number of neighbors to define size of Ai \n",
    "    - parameters_ell already contains knn, an object that has contains the number n_neighbors of neighbors for adaptive knn-based covariances computations on the calibration data. \n",
    "    '''\n",
    "    knn,y_pred_test,y_pred_train,local_alpha_s,lam,cov_train = parameters_ell\n",
    "    # choice of some points in Xtest, that will serve as centers to define subsets \n",
    "    random_idx = np.random.randint(len(Xtest),size=5) \n",
    "    # For each, define a neighborood Ai and compute empirical coverage on it \n",
    "    list_coverage_Ai = - np.ones(len(random_idx))\n",
    "    list_average_volume_Ai = - np.ones(len(random_idx))\n",
    "    for i,idx in enumerate(random_idx):\n",
    "        ### select nn-nearest neighbors \n",
    "        if len(x.shape)>1:\n",
    "            norms_ranks = np.linalg.norm(Xtest-Xtest[idx],axis=1)\n",
    "        else:\n",
    "            norms_ranks = np.abs(Xtest-Xtest[idx])\n",
    "        order = np.argsort(norms_ranks )\n",
    "        Ai = Xtest[order][:nn] \n",
    "        Yi = Ytest[order][:nn] \n",
    "        hatf_Xi = y_pred_test[order][:nn] \n",
    "        ### Evaluate coverage on Ai \n",
    "        local_neighbors_test = knn.kneighbors(Ai, return_distance=False) \n",
    "        normalized_ellipse_validity, normalized_ellipse_efficiency, = local_ellipse_validity_efficiency(\n",
    "                local_neighbors_test,\n",
    "                Yi,\n",
    "                hatf_Xi,\n",
    "                y,\n",
    "                y_pred_train,\n",
    "                local_alpha_s,\n",
    "                y.shape[1],\n",
    "                lam,\n",
    "                cov_train\n",
    "        )\n",
    "        list_coverage_Ai[i] = normalized_ellipse_validity /100\n",
    "        list_average_volume_Ai[i] = normalized_ellipse_efficiency\n",
    "\n",
    "    return( np.min(list_coverage_Ai),np.mean(list_average_volume_Ai) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Medium sample setting \n",
    "## Experiments are pursued with datasets of less than 1500 total observations (otherwise truncated). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated experiments \n",
    "########### LOAD DATA \n",
    "list_data = [\"enb\",\"atp1d\",\"jura\",\"rf1\",\"wq\",\"scm20d\",\"oes10\",\"oes97\"] \n",
    "dim_targets =  [2,6,7,8,14,16,16,16]\n",
    "\n",
    "\n",
    "list_data = [\"jura\"] \n",
    "dim_targets =  [7]\n",
    "\n",
    "alpha = 0.9\n",
    "t0 = time() \n",
    "\n",
    "Nrep = 10\n",
    "\n",
    "list_RESULTS_coverage = []\n",
    "list_RESULTS_coverage2 = []\n",
    "list_RESULTS_volume = []\n",
    "for m in range(len(dim_targets)):\n",
    "    # Load data \n",
    "    arff_file = arff.loadarff('./data/{}.arff'.format(list_data[m])) \n",
    "    df = pd.DataFrame(arff_file[0]).dropna()\n",
    "\n",
    "    # Separate explanatory and target variable (x,y) \n",
    "    df = np.array(df) \n",
    "    tt = df.shape[1] - dim_targets[m]\n",
    "    x = df[:1500,:tt] \n",
    "    y = df[:1500,tt:] \n",
    "    print(\"size calibration data:\",x.shape[0]/4)\n",
    "\n",
    "    # For robust results, repeat 'Nrep' times a random splitting of the data \n",
    "    WorstCoverage_rep = -np.ones(Nrep) \n",
    "    Average_volume_rep =  -np.ones(Nrep) \n",
    "    WorstCoverage_rep_ell =  -np.ones(Nrep) \n",
    "    Average_volume_rep_ell = -np.ones(Nrep) \n",
    "    MarginalCoverage = - np.ones(Nrep) \n",
    "    MarginalCoverage_ell = -np.ones(Nrep) \n",
    "    for rep in range(Nrep): \n",
    "        print(m,rep,\":\",np.round(time()-t0,3) )\n",
    "        #######################################\n",
    "        ## Split Train / Calibration / Test \n",
    "        #######################################\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "        X_test, X_cal, y_test, y_cal = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "        ## Learn regressor on Train data \n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        ## Evaluate scores on Calibration data and on Test data \n",
    "        y_pred_cal = model.predict(X_cal) \n",
    "        S_cal = y_cal - y_pred_cal \n",
    "        y_pred_test = model.predict(X_test) \n",
    "        Stest = y_test - y_pred_test \n",
    "\n",
    "        ## Choose a number of neighbors for KNN. Both methods use this, to estimate conditional distribution Y/X=x.\n",
    "        n_neighbors = int(len(X_cal)*0.1) \n",
    "\n",
    "        #######################################\n",
    "        # MARGINAL COVERAGE : \n",
    "        ########################################\n",
    "        NumberTestDataSet = len(X_test) # With low-medium datasets, this is the appropriate choice. Otherwise, take less than len(X_test) to accelerate computations (there is one OT problem for each Xtest, each OT problem being between point clouds with n_neighbors points)\n",
    "        indices = np.random.choice(len(X_test),NumberTestDataSet )\n",
    "\n",
    "        ## COMPUTE MK quantiles on calibration data and get metrics on test data \n",
    "        MarginalCoverage[rep],average_volume = SetCoverage(X_cal,S_cal,X_test[indices],Stest[indices],alpha=alpha,n=n_neighbors)\n",
    "        \n",
    "        ## COMPUTE ELLIPSOIDS on calibration data and get metrics on test data \n",
    "        MarginalCoverage_ell[rep], normalized_ellipse_efficiency = SetCoverage_ell(X_cal,y_cal,y_pred_cal,X_test[indices],y_test[indices],y_pred_test[indices],alpha,n_neighbors=n_neighbors)\n",
    "\n",
    "        ########################################\n",
    "        # WORST SET COVERAGE : \n",
    "        ########################################\n",
    "        ## Choose a number of neighbors for subsets Ai, when computing Worst Set coverage. \n",
    "        nn = int(len(X_cal)*0.1) # size of the five subsets to compute worst set coverage \n",
    "        WorstCoverage_test,list_average_volume_Ai = WorstSetCoverage(X_cal,S_cal,X_test,Stest,alpha=alpha,n=nn,n_neighbors=n_neighbors)\n",
    "        WorstCoverage_rep[rep] = WorstCoverage_test \n",
    "        Average_volume_rep[rep] = np.mean( list_average_volume_Ai ) \n",
    "        ## Compute Local ellipsoidal scores on calibration data and get metrics on test data \n",
    "        lam = 0.95 # lambda parameter in their paper \n",
    "        cov_train, alpha_s = ellipse_global_alpha_s(y_cal, y_pred_cal, y_cal, y_pred_cal, 1-alpha) \n",
    "        knn, local_alpha_s = ellipse_local_alpha_s(X_cal,X_cal,y_cal,y_pred_cal,y_cal,y_pred_cal,1-alpha,n_neighbors,lam,cov_train)\n",
    "        parameters_ell = [knn,y_pred_test,y_pred_cal,local_alpha_s,lam,cov_train]\n",
    "        WorstCoverage_rep_ell[rep], Average_volume_rep_ell[rep] = WorstSetCoverage_Ellipsoids(X_cal,y_cal,X_test,y_test,parameters_ell,nn=nn) \n",
    "\n",
    "\n",
    "    # For each data, we gather the results in a suitable form for future visualisations \n",
    "    WorstCoverages = np.concatenate([WorstCoverage_rep,WorstCoverage_rep_ell])\n",
    "    methods = np.concatenate([np.repeat(\"MK\",Nrep),np.repeat(\"Ell\",Nrep)])\n",
    "    name_data = np.repeat(list_data[m],Nrep*2)\n",
    "    df1 = np.array([WorstCoverages,methods,name_data]).T \n",
    "    list_RESULTS_coverage.append( df1 )\n",
    "\n",
    "    MarginalCoverages = np.concatenate([MarginalCoverage,MarginalCoverage_ell])\n",
    "    df1 = np.array([MarginalCoverages,methods,name_data]).T \n",
    "    list_RESULTS_coverage2.append( df1 )\n",
    "\n",
    "    Volumes = np.concatenate([Average_volume_rep,Average_volume_rep_ell])\n",
    "    df2 = np.array([Volumes,methods,name_data]).T \n",
    "    list_RESULTS_volume.append( df2 )\n",
    "    print(m,\":\",np.round(time()-t0,3) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# We transform results in DataFrame format:\n",
    "#############################################\n",
    "\n",
    "######### Worst Set Coverage \n",
    "df_results_coverage = list_RESULTS_coverage[0] \n",
    "for i in range(1,len(list_RESULTS_coverage)):\n",
    "    df_results_coverage = np.concatenate([df_results_coverage,list_RESULTS_coverage[i] ])\n",
    "df_results_coverage = pd.DataFrame(df_results_coverage,columns=[\"Coverage\",\"Method\",\"Data\"])\n",
    "df_results_coverage = df_results_coverage.astype({'Coverage': 'float'})\n",
    "\n",
    "######### Marginal Coverage  \n",
    "df_results_coverage2 = list_RESULTS_coverage2[0] \n",
    "for i in range(1,len(list_RESULTS_coverage2)):\n",
    "    df_results_coverage2 = np.concatenate([df_results_coverage2,list_RESULTS_coverage2[i] ])\n",
    "df_results_coverage2 = pd.DataFrame(df_results_coverage2,columns=[\"Coverage\",\"Method\",\"Data\"])\n",
    "df_results_coverage2 = df_results_coverage2.astype({'Coverage': 'float'})\n",
    "\n",
    "######### Average volume\n",
    "df_results_volume= list_RESULTS_volume[0] \n",
    "for i in range(1,len(list_RESULTS_volume)):\n",
    "    df_results_volume = np.concatenate([df_results_volume,list_RESULTS_volume[i] ])\n",
    "df_results_volume = pd.DataFrame(df_results_volume,columns=[\"Volume\",\"Method\",\"Data\"]) \n",
    "df_results_volume = df_results_volume.astype({'Volume': 'float'}) \n",
    "\n",
    "#################\n",
    "# We save the results \n",
    "df_results_coverage.to_csv('reg_mediumdata_realdata_WorstCoverages.csv', index=False) \n",
    "df_results_coverage2.to_csv('reg_mediumdata_realdata_MarginalCoverages.csv', index=False) \n",
    "df_results_volume.to_csv('reg_mediumdata_realdata_Volumes.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_coverage = pd.read_csv('reg_mediumdata_realdata_WorstCoverages.csv') \n",
    "df_results_coverage = pd.DataFrame(np.array(df_results_coverage),columns=[\"Worst Set Coverage\",\"Method\",\"Data\"]) \n",
    "df_results_coverage['Method'] = df_results_coverage['Method'].apply(lambda x: \"OT-CP\" if x ==\"MK\" else \"ELL\")\n",
    "df_results_coverage = df_results_coverage[df_results_coverage[\"Data\"]!=\"oes97\"]\n",
    "df_results_coverage = df_results_coverage[df_results_coverage[\"Data\"]!=\"oes10\"]\n",
    "\n",
    "plt.figure()\n",
    "g =sns.catplot( \n",
    "        data=df_results_coverage,  kind=\"bar\",\n",
    "        x=\"Data\", y=\"Worst Set Coverage\", hue=\"Method\", palette=[\"cornflowerblue\",\"lightpink\"],height=5,width=0.7,capsize=0.3,aspect=1.4,linewidth=2.5,legend=[\"OT-CP\",\"ELL\"]\n",
    ")   \n",
    "g.refline(y=alpha, color='black',linestyle=\"dashed\",linewidth=2)\n",
    "plt.savefig(\"RealData_RegressionConditionalCoverage.pdf\", format=\"pdf\",bbox_inches=\"tight\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "g =sns.catplot( \n",
    "        data=df_results_coverage,  kind=\"bar\",\n",
    "        x=\"Data\", y=\"Worst Set Coverage\", hue=\"Method\", palette=[\"cornflowerblue\",\"lightpink\"],height=5,width=0.7,capsize=0.3,aspect=1,linewidth=2.5,legend=[\"OT-CP\",\"ELL\"]\n",
    ")   \n",
    "g.refline(y=alpha, color='black',linestyle=\"dashed\",linewidth=2)\n",
    "plt.savefig(\"RealData_RegressionConditionalCoverage.pdf\", format=\"pdf\",bbox_inches=\"tight\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-data setting : \n",
    "## Two datasets of nearly 9000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated experiments \n",
    "########### LOAD DATA \n",
    "list_data = [\"rf1\",\"scm20d\"] \n",
    "dim_targets = [8,16]\n",
    "alpha = 0.9\n",
    "t0 = time() \n",
    "\n",
    "Nrep = 10\n",
    "list_RESULTS_coverage = []\n",
    "list_RESULTS_coverage2 = []\n",
    "list_RESULTS_volume = []\n",
    "for m in range(len(dim_targets)):\n",
    "    # Load data \n",
    "    arff_file = arff.loadarff('./data/{}.arff'.format(list_data[m])) \n",
    "    df = pd.DataFrame(arff_file[0]).dropna()\n",
    "\n",
    "    # Separate explanatory and target variable (x,y) \n",
    "    df = np.array(df) \n",
    "    tt = df.shape[1] - dim_targets[m]\n",
    "    x = df[:,:tt] #df[:1500,:tt] \n",
    "    y = df[:,tt:] #df[:1500,tt:] \n",
    "    print(\"size calibration data:\",x.shape[0]/4)\n",
    "\n",
    "    # For robust results, repeat 'Nrep' times a random splitting of the data \n",
    "    WorstCoverage_rep = -np.ones(Nrep) \n",
    "    Average_volume_rep =  -np.ones(Nrep) \n",
    "    WorstCoverage_rep_ell =  -np.ones(Nrep) \n",
    "    Average_volume_rep_ell = -np.ones(Nrep) \n",
    "    MarginalCoverage = - np.ones(Nrep) \n",
    "    MarginalCoverage_ell = -np.ones(Nrep) \n",
    "    for rep in range(Nrep): \n",
    "        print(m,rep,\":\",np.round(time()-t0,3) )\n",
    "        #######################################\n",
    "        ## Split Train / Calibration / Test \n",
    "        #######################################\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "        X_test, X_cal, y_test, y_cal = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "        ## Learn regressor on Train data \n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        ## Evaluate scores on Calibration data and on Test data \n",
    "        y_pred_cal = model.predict(X_cal) \n",
    "        S_cal = y_cal - y_pred_cal \n",
    "        y_pred_test = model.predict(X_test) \n",
    "        Stest = y_test - y_pred_test \n",
    "\n",
    "        ## Choose a number of neighbors for KNN. Both methods use this, to estimate conditional distribution Y/X=x.\n",
    "        n_neighbors = int(len(X_cal)*0.1) \n",
    "\n",
    "        #######################################\n",
    "        # MARGINAL COVERAGE : \n",
    "        ########################################\n",
    "        NumberTestDataSet = 200 # With low-medium datasets, this is the appropriate choice. Otherwise, take less than len(X_test) to accelerate computations (there is one OT problem for each Xtest, each OT problem being between point clouds with n_neighbors points)\n",
    "        indices = np.random.choice(len(X_test),NumberTestDataSet )\n",
    "\n",
    "        ## COMPUTE MK quantiles on calibration data and get metrics on test data \n",
    "        MarginalCoverage[rep],average_volume = SetCoverage(X_cal,S_cal,X_test[indices],Stest[indices],alpha=alpha,n=n_neighbors)\n",
    "        \n",
    "        ## COMPUTE ELLIPSOIDS on calibration data and get metrics on test data \n",
    "        MarginalCoverage_ell[rep], normalized_ellipse_efficiency = SetCoverage_ell(X_cal,y_cal,y_pred_cal,X_test[indices],y_test[indices],y_pred_test[indices],alpha,n_neighbors=n_neighbors)\n",
    "\n",
    "        ########################################\n",
    "        # WORST SET COVERAGE : \n",
    "        ########################################\n",
    "        ## Choose a number of neighbors for subsets Ai, when computing Worst Set coverage. \n",
    "        nn = int(len(X_cal)*0.1) # size of the five subsets to compute worst set coverage \n",
    "        WorstCoverage_test,list_average_volume_Ai = WorstSetCoverage(X_cal,S_cal,X_test,Stest,alpha=alpha,n=nn,n_neighbors=n_neighbors)\n",
    "        WorstCoverage_rep[rep] = WorstCoverage_test \n",
    "        Average_volume_rep[rep] = np.mean( list_average_volume_Ai ) \n",
    "        ## Compute Local ellipsoidal scores on calibration data and get metrics on test data \n",
    "        lam = 0.95 # lambda parameter in their paper \n",
    "        cov_train, alpha_s = ellipse_global_alpha_s(y_cal, y_pred_cal, y_cal, y_pred_cal, 1-alpha) \n",
    "        knn, local_alpha_s = ellipse_local_alpha_s(X_cal,X_cal,y_cal,y_pred_cal,y_cal,y_pred_cal,1-alpha,n_neighbors,lam,cov_train)\n",
    "        parameters_ell = [knn,y_pred_test,y_pred_cal,local_alpha_s,lam,cov_train]\n",
    "        WorstCoverage_rep_ell[rep], Average_volume_rep_ell[rep] = WorstSetCoverage_Ellipsoids(X_cal,y_cal,X_test,y_test,parameters_ell,nn=nn) \n",
    "\n",
    "\n",
    "    # For each data, we gather the results in a suitable form for future visualisations \n",
    "    WorstCoverages = np.concatenate([WorstCoverage_rep,WorstCoverage_rep_ell])\n",
    "    methods = np.concatenate([np.repeat(\"MK\",Nrep),np.repeat(\"Ell\",Nrep)])\n",
    "    name_data = np.repeat(list_data[m],Nrep*2)\n",
    "    df1 = np.array([WorstCoverages,methods,name_data]).T \n",
    "    list_RESULTS_coverage.append( df1 )\n",
    "\n",
    "    MarginalCoverages = np.concatenate([MarginalCoverage,MarginalCoverage_ell])\n",
    "    df1 = np.array([MarginalCoverages,methods,name_data]).T \n",
    "    list_RESULTS_coverage2.append( df1 )\n",
    "\n",
    "    Volumes = np.concatenate([Average_volume_rep,Average_volume_rep_ell])\n",
    "    df2 = np.array([Volumes,methods,name_data]).T \n",
    "    list_RESULTS_volume.append( df2 )\n",
    "    print(m,\":\",np.round(time()-t0,3) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# We transform results in DataFrame format:\n",
    "#############################################\n",
    "\n",
    "######### Worst Set Coverage \n",
    "df_results_coverage = list_RESULTS_coverage[0] \n",
    "for i in range(1,len(list_RESULTS_coverage)):\n",
    "    df_results_coverage = np.concatenate([df_results_coverage,list_RESULTS_coverage[i] ])\n",
    "df_results_coverage = pd.DataFrame(df_results_coverage,columns=[\"Coverage\",\"Method\",\"Data\"])\n",
    "df_results_coverage = df_results_coverage.astype({'Coverage': 'float'})\n",
    "\n",
    "######### Marginal Coverage  \n",
    "df_results_coverage2 = list_RESULTS_coverage2[0] \n",
    "for i in range(1,len(list_RESULTS_coverage2)):\n",
    "    df_results_coverage2 = np.concatenate([df_results_coverage2,list_RESULTS_coverage2[i] ])\n",
    "df_results_coverage2 = pd.DataFrame(df_results_coverage2,columns=[\"Coverage\",\"Method\",\"Data\"])\n",
    "df_results_coverage2 = df_results_coverage2.astype({'Coverage': 'float'})\n",
    "\n",
    "######### Average volume\n",
    "df_results_volume= list_RESULTS_volume[0] \n",
    "for i in range(1,len(list_RESULTS_volume)):\n",
    "    df_results_volume = np.concatenate([df_results_volume,list_RESULTS_volume[i] ])\n",
    "df_results_volume = pd.DataFrame(df_results_volume,columns=[\"Volume\",\"Method\",\"Data\"]) \n",
    "df_results_volume = df_results_volume.astype({'Volume': 'float'}) \n",
    "\n",
    "#################\n",
    "# We save the results \n",
    "df_results_coverage.to_csv('reg_manypoints_realdata_WorstCoverages.csv', index=False) \n",
    "df_results_coverage2.to_csv('reg_manypoints_realdata_MarginalCoverages.csv', index=False) \n",
    "df_results_volume.to_csv('reg_manypoints_realdata_Volumes.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_coverage = pd.read_csv('reg_manypoints_realdata_WorstCoverages.csv') \n",
    "df_results_coverage = pd.DataFrame(np.array(df_results_coverage),columns=[\"Worst Set Coverage\",\"Method\",\"Data\"]) \n",
    "df_results_coverage['Method'] = df_results_coverage['Method'].apply(lambda x: \"OT-CP\" if x ==\"MK\" else \"ELL\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.set_style(\"whitegrid\")\n",
    "g =sns.catplot( \n",
    "        data=df_results_coverage,  kind=\"bar\",\n",
    "        x=\"Data\", y=\"Worst Set Coverage\", hue=\"Method\", palette=[\"cornflowerblue\",\"lightpink\"],height=5,width=0.7,capsize=0.3,aspect=0.6,linewidth=2.5,legend=[\"OT-CP\",\"ELL\"]\n",
    ")   \n",
    "g.refline(y=alpha, color='black',linestyle=\"dashed\",linewidth=2)\n",
    "sns.move_legend(g, loc=\"center right\",fontsize='20',bbox_to_anchor=(1.1,.55),title_fontsize='15')\n",
    "plt.savefig(\"Realmanypoints_RegressionConditionalCoverage.pdf\", format=\"pdf\",bbox_inches=\"tight\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
